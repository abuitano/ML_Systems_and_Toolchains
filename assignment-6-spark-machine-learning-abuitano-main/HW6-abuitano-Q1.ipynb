{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee1bc8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.ml import Pipeline,Transformer\n",
    "from pyspark.ml.feature import Imputer,StandardScaler,StringIndexer,OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import numpy as np\n",
    "\n",
    "col_names = [\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\n",
    "\"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n",
    "\"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n",
    "\"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n",
    "\"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n",
    "\"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
    "\"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
    "\"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
    "\"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
    "\"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"class\",\"difficulty\"]\n",
    "\n",
    "nominal_cols = ['protocol_type','service','flag']\n",
    "binary_cols = ['land', 'logged_in', 'root_shell', 'su_attempted', 'is_host_login',\n",
    "'is_guest_login']\n",
    "continuous_cols = ['duration' ,'src_bytes', 'dst_bytes', 'wrong_fragment' ,'urgent', 'hot',\n",
    "'num_failed_logins', 'num_compromised', 'num_root' ,'num_file_creations',\n",
    "'num_shells', 'num_access_files', 'num_outbound_cmds', 'count' ,'srv_count',\n",
    "'serror_rate', 'srv_serror_rate' ,'rerror_rate' ,'srv_rerror_rate',\n",
    "'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate' ,'dst_host_count',\n",
    "'dst_host_srv_count' ,'dst_host_same_srv_rate' ,'dst_host_diff_srv_rate',\n",
    "'dst_host_same_src_port_rate' ,'dst_host_srv_diff_host_rate',\n",
    "'dst_host_serror_rate' ,'dst_host_srv_serror_rate', 'dst_host_rerror_rate',\n",
    "'dst_host_srv_rerror_rate']\n",
    "\n",
    "class OutcomeCreater(Transformer): # this defines a transformer that creates the outcome column\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        label_to_binary = udf(lambda name: 0.0 if name == 'normal' else 1.0)\n",
    "        output_df = dataset.withColumn('outcome', label_to_binary(col('class'))).drop(\"class\")  \n",
    "        output_df = output_df.withColumn('outcome', col('outcome').cast(DoubleType()))\n",
    "        output_df = output_df.drop('difficulty')\n",
    "        return output_df\n",
    "\n",
    "class FeatureTypeCaster(Transformer): # this transformer will cast the columns as appropriate types  \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        output_df = dataset\n",
    "        for col_name in binary_cols + continuous_cols:\n",
    "            output_df = output_df.withColumn(col_name,col(col_name).cast(DoubleType()))\n",
    "\n",
    "        return output_df\n",
    "class ColumnDropper(Transformer): # this transformer drops unnecessary columns\n",
    "    def __init__(self, columns_to_drop = None):\n",
    "        super().__init__()\n",
    "        self.columns_to_drop=columns_to_drop\n",
    "    def _transform(self, dataset):\n",
    "        output_df = dataset\n",
    "        for col_name in self.columns_to_drop:\n",
    "            output_df = output_df.drop(col_name)\n",
    "        return output_df\n",
    "\n",
    "def get_ML_pipeline():\n",
    "    # Stage where columns are casted as appropriate types\n",
    "    stage_typecaster = FeatureTypeCaster()\n",
    "\n",
    "    # Stage where nominal columns are transformed to index columns using StringIndexer\n",
    "    nominal_id_cols = [x+\"_index\" for x in nominal_cols]\n",
    "    nominal_onehot_cols = [x+\"_encoded\" for x in nominal_cols]\n",
    "    stage_nominal_indexer = StringIndexer(inputCols = nominal_cols, outputCols = nominal_id_cols )\n",
    "\n",
    "    # Stage where the index columns are further transformed using OneHotEncoder\n",
    "    stage_nominal_onehot_encoder = OneHotEncoder(inputCols=nominal_id_cols, outputCols=nominal_onehot_cols)\n",
    "\n",
    "    # Stage where all relevant features are assembled into a vector (and dropping a few)\n",
    "    feature_cols = continuous_cols+binary_cols+nominal_onehot_cols\n",
    "    corelated_cols_to_remove = [\"dst_host_serror_rate\",\"srv_serror_rate\",\"dst_host_srv_serror_rate\",\n",
    "                     \"srv_rerror_rate\",\"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\"]\n",
    "    for col_name in corelated_cols_to_remove:\n",
    "        feature_cols.remove(col_name)\n",
    "    stage_vector_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"vectorized_features\")\n",
    "\n",
    "    # Stage where we scale the columns\n",
    "    stage_scaler = StandardScaler(inputCol= 'vectorized_features', outputCol= 'features')\n",
    "    \n",
    "\n",
    "    # Stage for creating the outcome column representing whether there is attack \n",
    "    stage_outcome = OutcomeCreater()\n",
    "\n",
    "    # Removing all unnecessary columbs, only keeping the 'features' and 'outcome' columns\n",
    "    stage_column_dropper = ColumnDropper(columns_to_drop = nominal_cols+nominal_id_cols+\n",
    "        nominal_onehot_cols+ binary_cols + continuous_cols + ['vectorized_features'])\n",
    "    \n",
    "    # Creating the Logistic Regression ML model\n",
    "    lr = LogisticRegression(featuresCol = 'features', labelCol = 'outcome')\n",
    "    \n",
    "    # Parameter grid for Cross Validator\n",
    "    lr_paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.01, 0.5, 2.0])\n",
    "             .addGrid(lr.maxIter, [1, 5, 10])\n",
    "             .build())\n",
    "    \n",
    "    # Selecting the evaluator.\n",
    "    evaluator = BinaryClassificationEvaluator(rawPredictionCol='rawPrediction', \n",
    "                        labelCol='outcome', metricName='areaUnderROC')\n",
    "    \n",
    "    # Stage for Cross Validation\n",
    "    cross_validator = CrossValidator(estimator=lr, estimatorParamMaps=lr_paramGrid, \n",
    "                    evaluator=evaluator, numFolds=5)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Connect the columns into a pipeline\n",
    "    pipeline = Pipeline(stages=[stage_typecaster,stage_nominal_indexer,stage_nominal_onehot_encoder,\n",
    "        stage_vector_assembler,stage_scaler,stage_outcome,stage_column_dropper, cross_validator])\n",
    "    return pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "198ea5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/23 19:46:06 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                          (0 + 5) / 5]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/23 19:46:11 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/23 19:46:17 WARN InstanceBuilder$JavaBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"GenericAppName\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "nslkdd_raw = spark.read.csv('../NSL-KDD/KDDTrain+.txt',header=False).toDF(*col_names)\n",
    "nslkdd_test_raw = spark.read.csv('../NSL-KDD/KDDTest+.txt',header=False).toDF(*col_names)\n",
    "\n",
    "ML_pipeline = get_ML_pipeline()\n",
    "ML_model = ML_pipeline.fit(nslkdd_raw)\n",
    "\n",
    "# nslkdd_df = ML_model.transform(nslkdd_raw)\n",
    "test_predictions = ML_model.transform(nslkdd_test_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49e3026e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- outcome: double (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c90aa6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------------------+--------------------+----------+\n",
      "|            features|outcome|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-------+--------------------+--------------------+----------+\n",
      "|(113,[13,14,16,17...|    1.0|[-4.3653326088596...|[0.01255089958022...|       1.0|\n",
      "|(113,[13,14,16,17...|    1.0|[-3.9491045312633...|[0.01890756579045...|       1.0|\n",
      "|(113,[0,1,13,14,1...|    0.0|[1.79696740987287...|[0.85777937880511...|       0.0|\n",
      "|(113,[1,13,14,17,...|    1.0|[-2.3171179288349...|[0.08971514939145...|       1.0|\n",
      "|(113,[0,2,13,14,1...|    1.0|[0.21059847464018...|[0.55245588610067...|       0.0|\n",
      "+--------------------+-------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "22/10/25 05:07:33 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 1035903 ms exceeds timeout 120000 ms\n",
      "22/10/25 05:07:33 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "test_predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1a0369",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
